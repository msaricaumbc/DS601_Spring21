{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/msarica/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/msarica/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "# import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_model_file(filename):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     with open(filename, 'wb') as data:\n",
    "#         s3.Bucket(\"com.msarica.ds\").download_fileobj(filename, data)\n",
    "\n",
    "def load_model_file(filename):\n",
    "    obj = pickle.load(open(filename, 'rb'))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.pickle'\n",
    "\n",
    "# if os.path.exists(filename) == False:\n",
    "#     print('model not found')\n",
    "#     get_model_file(filename)\n",
    "#     print('model downloaded from s3')\n",
    "# else:\n",
    "#     print('model file found!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = load_model_file(filename)\n",
    "\n",
    "model = obj['model']\n",
    "stopwords = obj['stopwords']\n",
    "lemmatizer = obj ['lemmatizer']\n",
    "wordnet_map = obj ['wordnet_map']\n",
    "vectorizer = obj ['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction pipeline with text vectorizer and model\n",
    "def prediction(text_input, STOPWORDS, lemmatizer, wordnet_map, vectorizer, Multinomial_NB):\n",
    "    \n",
    "    # Transform words to lower case\n",
    "    text=text_input.lower()\n",
    "    # Remove punctuation\n",
    "    text=re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    # Remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    # Remove stop words like : and is a and the\n",
    "    text=\" \".join([word for word in text.split() if word not in STOPWORDS])\n",
    "    # Find base word for all words in the sentence\n",
    "    pos_tagged_text=nltk.pos_tag(text.split())\n",
    "    text=\" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    \n",
    "    # specific\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text); text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text); text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text); text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text); text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text); text = re.sub(r\"\\'m\", \" am\", text)\n",
    "#     print(text)\n",
    "    text = np.array(text).reshape(1,)\n",
    "    vec_text = vectorizer.transform(text)\n",
    "    output = Multinomial_NB.predict(vec_text)\n",
    "    \n",
    "    cols = ['emotion_0','emotion_1','emotion_2','emotion_3','emotion_4',\n",
    "            'emotion_5','emotion_6','emotion_7','emotion_8']\n",
    "    output_values = [False if x == 0 else True for x in output[0]]  # np values are replaced with boolean\n",
    " \n",
    "    return { key: value for key, value in zip(cols, output_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion_0': False,\n",
       " 'emotion_1': True,\n",
       " 'emotion_2': False,\n",
       " 'emotion_3': False,\n",
       " 'emotion_4': False,\n",
       " 'emotion_5': True,\n",
       " 'emotion_6': False,\n",
       " 'emotion_7': True,\n",
       " 'emotion_8': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Source[BBC]: https://www.bbc.com/news/world-asia-56546920\n",
    "# text_input= \"Myanmar coup: Dozens killed as army opens fire on protesters during deadliest day.Security forces were out in strength trying to prevent rallies.Local news site Myanmar Now put the death toll at 114, while the United Nations said it was receiving reports of scores killed and hundreds more injured.\"\n",
    "\n",
    "# #Prediction output\n",
    "# output= prediction(text_input, stopwords, lemmatizer, wordnet_map, vectorizer, model)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(sentence):\n",
    "    return prediction(sentence, stopwords, lemmatizer, wordnet_map, vectorizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/Apr/2021 20:45:52] \"\u001b[37mGET /?sentence=today%20is%20a%20good%20day HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Apr/2021 20:46:36] \"\u001b[37mGET /?sentence=The%20US%20has%20warned%20Russia%20there%20will%20be%20%22consequences%22%20if%20the%20opposition%20activist%20Alexei%20Navalny%20dies%20in%20jail. HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/')\n",
    "def start():\n",
    "    sentence = request.args.get('sentence')\n",
    "\n",
    "    if sentence == None: \n",
    "        return jsonify({})\n",
    "\n",
    "    # print(sentence)\n",
    "    \n",
    "    result = make_prediction(sentence)\n",
    "    \n",
    "    # print(result)\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "from flask_cors import CORS\n",
    "from waitress import serve\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/')\n",
    "def start():\n",
    "    sentence = request.args.get('sentence')\n",
    "\n",
    "    if sentence == None: \n",
    "        return jsonify({})\n",
    "\n",
    "    # print(sentence)\n",
    "    \n",
    "    result = make_prediction(sentence)\n",
    "    \n",
    "    # print(result)\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    serve(app, host=\"0.0.0.0\", port=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
